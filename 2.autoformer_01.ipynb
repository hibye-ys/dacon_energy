{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d5cdc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480b2039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/youngseoklee/Desktop/workplace/dacon_energy'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66e3a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/youngseoklee/Desktop/workplace/datas/dacon_energy/\n"
     ]
    }
   ],
   "source": [
    "#파일경로 설정\n",
    "if 'macOS' in platform.platform():\n",
    "    path = '/Users/youngseoklee/Desktop/workplace/datas/dacon_energy/'\n",
    "    print(path)\n",
    "elif 'Linux' in platform.platform():\n",
    "    path = '/workplace/datas/dacon_energy/'\n",
    "    print(path)\n",
    "else:\n",
    "    print('어느 os에도 속해있지 않습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4952bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "#디바이스 설정\n",
    "if 'macOS' in platform.platform():\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    print(device)\n",
    "elif 'Linux' in platform.platform():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "else:\n",
    "    print('어느 os에도 속해있지 않습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c34f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path + 'train_df.csv', index_col=0)\n",
    "test_df = pd.read_csv(path + 'test_df.csv', index_col=0)\n",
    "sub = pd.read_csv(path + 'sample_submission.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc50a286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((204000, 29), (16800, 28))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape , test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "403c93e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['건물번호', '기온(C)', '강수량(mm)', '풍속(m/s)', '습도(%)', '전력소비량(kWh)', 'Year',\n",
       "       'Month', 'Day', 'Hour', '연면적(m2)', '냉방면적(m2)', '태양광용량(kW)',\n",
       "       'ESS저장용량(kWh)', 'PCS용량(kW)', '건물유형_건물기타', '건물유형_공공', '건물유형_대학교',\n",
       "       '건물유형_데이터센터', '건물유형_백화점및아울렛', '건물유형_병원', '건물유형_상용', '건물유형_아파트',\n",
       "       '건물유형_연구소', '건물유형_지식산업센터', '건물유형_할인마트', '건물유형_호텔및리조트', '요일', '주말'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d41857",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebbeca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Transformer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae8cabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 24\n",
    "TRAIN_SIZE = 0.8\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4c124d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Select all columns and convert to numpy array\n",
    "X = train_df.drop('전력소비량(kWh)', axis=1).values\n",
    "y = train_df['전력소비량(kWh)'].values\n",
    "\n",
    "\n",
    "\n",
    "# Generate sequences\n",
    "X = [X[i:i+SEQUENCE_LENGTH, :] for i in range(X.shape[0]-SEQUENCE_LENGTH)]\n",
    "y = [y[i+SEQUENCE_LENGTH] for i in range(y.shape[0]-SEQUENCE_LENGTH)]\n",
    "X, y = np.array(X), np.array(y).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3161c3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((203976, 24, 26), (203976, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape\n",
    "\n",
    "# batch, seq_len, features /// "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8400667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0~203976\n",
    "\n",
    "#data[0:24, : -1]\n",
    "#data[1:25, : -1]\n",
    "#.....\n",
    "#data[203975 : 2~ , :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f9ab4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation datasets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=TRAIN_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f319806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to PyTorch tensors and create dataloaders\n",
    "train_data = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())\n",
    "val_data = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float())\n",
    "test_data = TensorDataset(torch.from_numpy(test_df.values).float())\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "411525af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((163180, 24, 26), (40796, 24, 26), (163180, 1), (40796, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(B, Seq_len, C) (B, C)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c1134",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from math import sqrt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with the following two phases:\n",
    "    (1) period-based dependencies discovery\n",
    "    (2) time delay aggregation\n",
    "    This block can replace the self-attention family mechanism seamlessly.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor #??\n",
    "        self.scale = scale #??\n",
    "        self.mask_flag = mask_flag #??\n",
    "        self.output_attention = output_attention ##??\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def time_delay_agg_training(self, values, corr):\n",
    "        \"\"\"\n",
    "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
    "        This is for the training phase.\n",
    "        \"\"\"\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        index = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]\n",
    "        weights = torch.stack([mean_value[:, index[i]] for i in range(top_k)], dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            pattern = torch.roll(tmp_values, -int(index[i]), -1)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_inference(self, values, corr):\n",
    "        \"\"\"\n",
    "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
    "        This is for the inference phase.\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0).repeat(batch, head, channel, 1).cuda()\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        weights = torch.topk(mean_value, top_k, dim=-1)[0]\n",
    "        delay = torch.topk(mean_value, top_k, dim=-1)[1]\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            tmp_delay = init_index + delay[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_full(self, values, corr):\n",
    "        \"\"\"\n",
    "        Standard version of Autocorrelation\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0).repeat(batch, head, channel, 1).cuda()\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        weights = torch.topk(corr, top_k, dim=-1)[0]\n",
    "        delay = torch.topk(corr, top_k, dim=-1)[1]\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            tmp_delay = init_index + delay[..., i].unsqueeze(-1)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * (tmp_corr[..., i].unsqueeze(-1))\n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "\n",
    "        # period-based dependencies\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "\n",
    "        # time delay agg\n",
    "        if self.training:\n",
    "            V = self.time_delay_agg_training(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            V = self.time_delay_agg_inference(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57dceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, correlation, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_correlation = correlation\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_correlation(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0302151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58289e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
